{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Mathijn\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Mathijn\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Mathijn\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt','wordnet','stopwords'])\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "engine = create_engine('sqlite:///..\\data\\DisasterResponse.db')\n",
    "df = pd.read_sql_table('messages', engine) \n",
    "X = df.message\n",
    "Y = df.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization function to process the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(message):    \n",
    "\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+' \n",
    "          \n",
    "    # Removing any URLs\n",
    "    detected_urls = re.findall(url_regex, message)\n",
    "\n",
    "    for url in detected_urls:\n",
    "        message = message.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    # Removing punctuation\n",
    "    message = re.sub(r\"[^a-zA-Z0-9]\", \" \", message) \n",
    "\n",
    "    # Tokenizing the message\n",
    "    tokens = word_tokenize(message)           \n",
    "\n",
    "    # Lemmatize, lowercase, strip and also removing stopwords    \n",
    "    clean_tokens = [WordNetLemmatizer().lemmatize(t).lower().strip() for t in tokens if t not in stopwords.words(\"english\")]\n",
    "\n",
    "    return(clean_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline takea in the `message` column as input and output classification results on the other 36 categories in the dataset. The countvectorizer calls the tokenizer function defined above. The transformed data is passed to the TfidTransformer and finally the data is fitted with a random forest classifier. The multi output classifier is used because the categories are not mutually exlusive: more labels can exist for each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier())) \n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a randomized search of a parameter grid in order to keep computation time down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cv(pipeline):\n",
    "    \n",
    "    parameters = {   \n",
    "        'vect__ngram_range': [(1,1),(1,2),(2,2)],     \n",
    "        'vect__max_df': (0.25, 0.5, 0.75, 1),\n",
    "        'vect__max_features': (None, 2000, 4000, 6000, 8000, 10000),\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'clf__estimator__n_estimators': [10, 40, 70, 100, 130, 160, 190, 230],\n",
    "        'clf__estimator__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "        'clf__estimator__max_features': ['auto', 'sqrt'], \n",
    "        'clf__estimator__min_samples_leaf': [1, 2, 4],\n",
    "        'clf__estimator__min_samples_split': [2, 5, 10],\n",
    "        'clf__estimator__bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    cv = RandomizedSearchCV(pipeline, \n",
    "            param_distributions = parameters,\n",
    "            scoring = ['f1_micro','precision_micro','recall_micro'],\n",
    "            refit ='f1_micro',\n",
    "            verbose = 3,\n",
    "            n_jobs = -1,\n",
    "            n_iter = 80      \n",
    "            )\n",
    "\n",
    "    return(cv)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed: 19.2min\n[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed: 94.8min\n[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 245.8min\n[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 353.5min finished\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'vect__ngram_range': (1, 1),\n 'vect__max_features': 2000,\n 'vect__max_df': 0.75,\n 'tfidf__use_idf': True,\n 'clf__estimator__n_estimators': 160,\n 'clf__estimator__min_samples_split': 10,\n 'clf__estimator__min_samples_leaf': 1,\n 'clf__estimator__max_features': 'sqrt',\n 'clf__estimator__max_depth': 80,\n 'clf__estimator__bootstrap': True}"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, \n",
    "    test_size=0.2, \n",
    "    random_state=42)\n",
    "\n",
    "model = random_cv(pipeline)\n",
    "model.fit(X_train, y_train) \n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test model\n",
    "The precision, recall, and F1 score of the best model of the random search are shown for each of the labels. Also, the best fitting parameters are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n               related       0.82      0.96      0.89      3998\n               request       0.84      0.49      0.62       891\n                 offer       0.00      0.00      0.00        24\n           aid_related       0.76      0.68      0.72      2164\n          medical_help       0.68      0.14      0.23       435\n      medical_products       0.74      0.14      0.24       279\n     search_and_rescue       0.55      0.08      0.14       136\n              security       0.00      0.00      0.00        96\n              military       0.59      0.11      0.18       158\n                 water       0.86      0.52      0.65       335\n                  food       0.85      0.75      0.80       584\n               shelter       0.80      0.52      0.63       468\n              clothing       0.76      0.19      0.30        70\n                 money       0.77      0.09      0.16       112\n        missing_people       1.00      0.02      0.03        63\n              refugees       0.58      0.08      0.14       170\n                 death       0.84      0.28      0.42       247\n             other_aid       0.67      0.05      0.09       692\ninfrastructure_related       0.67      0.01      0.01       336\n             transport       0.74      0.11      0.19       235\n             buildings       0.89      0.21      0.34       269\n           electricity       1.00      0.08      0.15       115\n                 tools       0.00      0.00      0.00        35\n             hospitals       0.00      0.00      0.00        52\n                 shops       0.00      0.00      0.00        25\n           aid_centers       0.00      0.00      0.00        64\n  other_infrastructure       0.00      0.00      0.00       225\n       weather_related       0.85      0.74      0.79      1472\n                floods       0.91      0.52      0.67       431\n                 storm       0.76      0.62      0.69       479\n                  fire       0.00      0.00      0.00        53\n            earthquake       0.89      0.83      0.86       515\n                  cold       0.64      0.13      0.22       104\n         other_weather       0.75      0.07      0.12       267\n         direct_report       0.80      0.35      0.49      1010\n\n             micro avg       0.81      0.57      0.67     16609\n             macro avg       0.60      0.25      0.31     16609\n          weighted avg       0.77      0.57      0.60     16609\n           samples avg       0.67      0.51      0.53     16609\n\n"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred,target_names=Y.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve model\n",
    "Use grid search to find better parameters. Here we are finetuning the best parameters found by the randomsearch to see if we can improve model fit further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_cv(pipeline):\n",
    "    \n",
    "    parameters = {   \n",
    "        'vect__ngram_range': [(1,1)],     \n",
    "        'vect__max_df': (0.7, 0.75, 0.8),\n",
    "        'vect__max_features': (1000, 2000, 3000),\n",
    "        'clf__estimator__n_estimators': [160],\n",
    "        'clf__estimator__max_features': [\"sqrt\"], \n",
    "        'clf__estimator__min_samples_split': [5] \n",
    "    }\n",
    "\n",
    "    cv = GridSearchCV(pipeline,\n",
    "        param_grid = parameters,\n",
    "        scoring = ['f1_micro','precision_micro','recall_micro'],\n",
    "        refit ='f1_micro',\n",
    "        verbose = 3,\n",
    "        n_jobs = 4                  \n",
    "        )\n",
    "\n",
    "    return(cv)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed: 52.5min\n[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed: 103.3min finished\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'clf__estimator__max_features': 'sqrt',\n 'clf__estimator__min_samples_split': 5,\n 'clf__estimator__n_estimators': 160,\n 'vect__max_df': 0.7,\n 'vect__max_features': 2000,\n 'vect__ngram_range': (1, 1)}"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "model = grid_cv(pipeline)\n",
    "model.fit(X_train, y_train) \n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test model\n",
    "Show the accuracy, precision, and recall of the tuned model. The improvements are marginal over the best model from the randomsearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n               related       0.85      0.93      0.89      3998\n               request       0.82      0.50      0.62       891\n                 offer       0.00      0.00      0.00        24\n           aid_related       0.74      0.71      0.73      2164\n          medical_help       0.67      0.16      0.26       435\n      medical_products       0.68      0.16      0.26       279\n     search_and_rescue       0.56      0.10      0.17       136\n              security       0.50      0.01      0.02        96\n              military       0.59      0.11      0.18       158\n                 water       0.83      0.56      0.66       335\n                  food       0.84      0.76      0.80       584\n               shelter       0.80      0.53      0.64       468\n              clothing       0.88      0.21      0.34        70\n                 money       0.69      0.08      0.14       112\n        missing_people       1.00      0.03      0.06        63\n              refugees       0.66      0.14      0.22       170\n                 death       0.83      0.38      0.52       247\n             other_aid       0.60      0.07      0.13       692\ninfrastructure_related       0.00      0.00      0.00       336\n             transport       0.67      0.12      0.20       235\n             buildings       0.84      0.22      0.34       269\n           electricity       1.00      0.07      0.13       115\n                 tools       0.00      0.00      0.00        35\n             hospitals       0.00      0.00      0.00        52\n                 shops       0.00      0.00      0.00        25\n           aid_centers       0.00      0.00      0.00        64\n  other_infrastructure       0.00      0.00      0.00       225\n       weather_related       0.84      0.76      0.79      1472\n                floods       0.90      0.53      0.67       431\n                 storm       0.74      0.65      0.69       479\n                  fire       0.50      0.02      0.04        53\n            earthquake       0.89      0.81      0.85       515\n                  cold       0.61      0.13      0.22       104\n         other_weather       0.58      0.08      0.14       267\n         direct_report       0.77      0.37      0.50      1010\n\n             micro avg       0.81      0.57      0.67     16609\n             macro avg       0.60      0.26      0.32     16609\n          weighted avg       0.75      0.57      0.61     16609\n           samples avg       0.64      0.50      0.51     16609\n\n"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred,target_names=Y.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}